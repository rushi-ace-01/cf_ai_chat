Cloudflare AI Chat App (Workers AI + KV + Pages)

This is my Cloudflare assignment project: a simple full-stack AI chat application built using Cloudflareâ€™s developer platform.
It includes:

LLM: Llama 3.3 (via Cloudflare Workers AI)

Workflow / coordination: Cloudflare Worker (backend)

User input: A browser-based chat UI (HTML + JS)

Memory / state: Cloudflare KV for conversation history per session

ğŸš€ Architecture Overview
1. Backend â€“ Cloudflare Worker

Located in: /backend

Responsibilities:

Accept POST requests from the chat UI

Read user messages and session ID

Load conversation history from KV

Call Llama 3.3 using Workers AI

Save the updated chat history

Return assistant message as JSON

2. Workers AI Integration

Uses the model:

@cf/meta/llama-3.3-70b-instruct-fp8-fast


Called through:

const aiResp = await env.AI.run(model, { messages });

3. Memory / State via KV

Each browser tab gets its own random sessionId.

History is stored as:

CHAT_HISTORY[sessionId] => JSON array of messages


This satisfies the Cloudflare requirement for stateful behavior.

4. Frontend Chat UI

Located in: /frontend/index.html

Features:

Simple chat interface

Sends user input to Worker using fetch()

Displays AI responses

Maintains same sessionId across messages

ğŸ“¦ Folder Structure
cf-ai-chat/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ wrangler.jsonc
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ src/index.js
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html
â”‚
â””â”€â”€ README.md

ğŸ›  Running Locally
Backend:
cd backend
wrangler dev

Frontend:

Open frontend/index.html in a browser
(or upload to Cloudflare Pages).

ğŸŒ Deployment
Deploy Worker:
wrangler deploy

Deploy Frontend (optional):

Upload the /frontend folder to Cloudflare Pages.